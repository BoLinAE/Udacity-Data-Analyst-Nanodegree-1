{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def write_data(file, data, header):\n",
    "    with open(file, 'w') as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\",\", fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        good_data = []\n",
    "        bad_data = []\n",
    "        for row in reader:\n",
    "            # Only want data from dbpedia.org, do not put other data into either file\n",
    "            if (row['URI'] == 'URI') or row['URI'] == ('http://www.w3.org/2002/07/owl#Thing'):\n",
    "                pass\n",
    "            else:\n",
    "                production_start_year = row['productionStartYear'][0:4]\n",
    "                try:\n",
    "                    production_start_year = int(production_start_year)\n",
    "                    if (1886 < production_start_year < 2014):\n",
    "                        row['productionStartYear'] = production_start_year\n",
    "                        good_data.append(row)\n",
    "                    else:\n",
    "                        bad_data.append(row)\n",
    "                except:\n",
    "                    bad_data.append(row)\n",
    "\n",
    "    write_data(output_good, good_data, header)\n",
    "    write_data(output_bad, bad_data, header)\n",
    "\n",
    "process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exString = 'this is some text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exString.find('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exString.find('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CITIES = 'cities.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areaCode': {int, str, NoneType},\n",
       " 'areaLand': {list, NoneType, float},\n",
       " 'areaMetro': {NoneType, float},\n",
       " 'areaUrban': {NoneType, float},\n",
       " 'elevation': {list, NoneType, float},\n",
       " 'governmentType_label': {str, NoneType},\n",
       " 'homepage': {str, NoneType},\n",
       " 'isPartOf_label': {list, NoneType, str},\n",
       " 'maximumElevation': {NoneType},\n",
       " 'minimumElevation': {NoneType},\n",
       " 'name': {str, NoneType, list},\n",
       " 'populationDensity': {list, NoneType, float},\n",
       " 'populationTotal': {int, NoneType},\n",
       " 'timeZone_label': {str, NoneType},\n",
       " 'utcOffset': {int, str, NoneType, list},\n",
       " 'wgs84_pos#lat': {float},\n",
       " 'wgs84_pos#long': {float}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def determine_type(data):\n",
    "    if (data == 'NULL') or (data == \"\"):\n",
    "        return type(None)\n",
    "    elif data[0] == \"{\":\n",
    "        return type([])\n",
    "    else:\n",
    "        try:\n",
    "            float(data)\n",
    "            try:\n",
    "                int(data)\n",
    "                return type(int())\n",
    "            except:\n",
    "                return type(float())\n",
    "        except:\n",
    "            return type(str())\n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        headers = reader.fieldnames\n",
    "        # Create empty dictionary where each value is a set of data types\n",
    "        # The keys of the dictionary are the fields\n",
    "        for field in fields:\n",
    "            type_dict[field] = set([])\n",
    "            \n",
    "        for row in reader:\n",
    "            if row['URI'].find('dbpedia.org') < 0:\n",
    "                    pass\n",
    "            else:\n",
    "                for field in fields:\n",
    "                    type_dict[field].add(determine_type(row[field]))\n",
    "\n",
    "    return type_dict\n",
    "\n",
    "audit_file(CITIES,FIELDS)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CITIES = 'cities.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_area(area):\n",
    "    # Remove the braces from area\n",
    "    area = area.strip(\"{}\")\n",
    "    \n",
    "    if area == 'NULL':\n",
    "        return None\n",
    "    else:\n",
    "        #split on the pipe character\n",
    "        area = area.split(\"|\")\n",
    "        try:\n",
    "            return float(area)\n",
    "        except:\n",
    "            return(float(max(area, key=len)))\n",
    "                \n",
    "def fix_name(name):\n",
    "    # Remove the braces from name\n",
    "    name= name.strip(\"{}\")\n",
    "    \n",
    "    if name == \"NULL\":\n",
    "        return []\n",
    "    else:\n",
    "        name = name.split(\"|\")\n",
    "        return name\n",
    "\n",
    "def check_loc(point, lat, longi):\n",
    "    point = point.split(\" \")\n",
    "    if (point[0] == lat) and (point[1]==longi):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        headers = reader.fieldnames\n",
    "        #skipping the extra metadata\n",
    "        for row in reader:\n",
    "            if row['URI'].find('dbpedia.org') < 0:\n",
    "                pass\n",
    "\n",
    "        # processing file\n",
    "            else:\n",
    "                row['Point Correct'] = False\n",
    "            # calling your function to fix the area value\n",
    "                if \"areaLand\" in row:\n",
    "                    \n",
    "                    row[\"areaLand\"] = fix_area(row['areaLand'])\n",
    "                \n",
    "                \n",
    "                if \"name\" in row:\n",
    "                    row['name'] = fix_name(row['name'])\n",
    "                    \n",
    "                if 'point' in row:\n",
    "                    row['Point Correct'] == check_loc(row['point'], row[\"wgs84_pos#lat\"], row[\"wgs84_pos#long\"])\n",
    "                    \n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "\n",
    "cities_data = process_file(CITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
