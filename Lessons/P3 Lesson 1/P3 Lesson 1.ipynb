{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    data = [[sheet.cell_value(r,col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    \n",
    "    print('\\nList Comprehension')\n",
    "    print('data[4][3]:')\n",
    "    print(data[4][3])\n",
    "    \n",
    "    print('\\nCells in a nested loop:')\n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print(sheet.cell_value(row,col))\n",
    "                \n",
    "    print(\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "    print( \"Number of rows in the sheet:\" )\n",
    "    print( sheet.nrows)\n",
    "    print (\"Type of data in cell (row 3, col 2):\")\n",
    "    print( sheet.cell_type(3, 2))\n",
    "    print( \"Value in cell (row 3, col 2):\" )\n",
    "    print( sheet.cell_value(3, 2))\n",
    "    print( \"Get a slice of values in column 3, from rows 1-3:\")\n",
    "    print( sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "\n",
    "    print( \"\\nDATES:\")\n",
    "    print( \"Type of data in cell (row 1, col 0):\")\n",
    "    print( sheet.cell_type(1, 0))\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print( \"Time in Excel format:\")\n",
    "    print( exceltime)\n",
    "    print( \"Convert time to a Python datetime tuple, from the Excel float:\",)\n",
    "    print( xlrd.xldate_as_tuple(exceltime, 0))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    coast_data = sheet.col_values(1, 1)\n",
    "    max_load = max(coast_data)\n",
    "    max_index = coast_data.index(max_load)\n",
    "    max_time = sheet.cell_value(max_index + 1, 0)\n",
    "    max_time = xlrd.xldate_as_tuple(max_time, 0)\n",
    "    min_load = min(coast_data)\n",
    "    min_index = coast_data.index(min_load)\n",
    "    min_time = sheet.cell_value(min_index + 1, 0)\n",
    "    min_time = xlrd.xldate_as_tuple(min_time, 0)\n",
    "    average_load = sum(coast_data)/len(coast_data)\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': (0, 0, 0, 0, 0, 0),\n",
    "            'maxvalue': 0,\n",
    "            'mintime': (0, 0, 0, 0, 0, 0),\n",
    "            'minvalue': 0,\n",
    "            'avgcoast': 0\n",
    "    }\n",
    "    data['maxtime'] = max_time\n",
    "    data['maxvalue'] = round(max_load, 2)\n",
    "    data['mintime'] = min_time\n",
    "    data['minvalue'] = round(min_load, 2)\n",
    "    data['avgcoast'] = round(average_load, 2)\n",
    "    return data\n",
    "\n",
    "print(parse_file(datafile) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print( \"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print( json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"First Aid Kit\")\n",
    "    artists = results['artists']\n",
    "    counter = 0\n",
    "    for band in artists:\n",
    "        \n",
    "        if band['name'] == 'First Aid Kit':\n",
    "            print(band)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    # print( \"\\nARTIST:\")\n",
    "    #pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    # print( \"\\nONE RELEASE:\")\n",
    "    #pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    \n",
    "\n",
    "    # print( \"\\nALL TITLES:\")\n",
    "    for t in release_titles:\n",
    "        print(t)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = '745090.csv'\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'r') as f:\n",
    "        r = csv.reader(f, delimiter=\",\")\n",
    "        station = next(r)[1]\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            data.append(row)\n",
    "        \n",
    "    # Do not change the line below\n",
    "    name = station\n",
    "    return (name, data)\n",
    "\n",
    "parse_file(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COAST': [2013, 8, 13, 17, 18779.025510000003], 'EAST': [2013, 8, 5, 17, 2380.1654089999956], 'FAR_WEST': [2013, 6, 26, 17, 2281.2722140000024], 'NORTH': [2013, 8, 7, 17, 1544.7707140000005], 'NORTH_C': [2013, 8, 7, 18, 24415.570226999993], 'SOUTHERN': [2013, 8, 8, 16, 5494.157645], 'SOUTH_C': [2013, 8, 8, 18, 11433.30491600001], 'WEST': [2013, 8, 7, 17, 1862.6137649999998]}\n"
     ]
    }
   ],
   "source": [
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    station_dict = {}\n",
    "    for i in range(1, sheet.ncols-1):\n",
    "        \n",
    "        # Station names are in row 0, starting at column 1\n",
    "        station_name = sheet.cell_value(0, i)\n",
    "\n",
    "        # Retrieve the data, starts in column 1 row 1\n",
    "        # sheet.col_values(column, starting_row, ending_row=None)\n",
    "        data = sheet.col_values(i, 1)\n",
    "        \n",
    "        # Retrieve maximum load in the column (station)\n",
    "        max_load = max(data)\n",
    "        \n",
    "        # Retrieve index of maximum load (time)\n",
    "        max_index = data.index(max_load)\n",
    "        \n",
    "        # Retrieve time of maximum load (increase row by 1), convert time to tuple\n",
    "        # sheet.cell_value(row, column)\n",
    "        max_time = sheet.cell_value(max_index+1, 0)\n",
    "        max_time = xlrd.xldate_as_tuple(max_time, 0)\n",
    "        \n",
    "        max_year = max_time[0]\n",
    "        max_month = max_time[1]\n",
    "        max_day = max_time[2]\n",
    "        max_hour = max_time[3]\n",
    "        \n",
    "        station_dict[station_name] = ([max_year, max_month, max_day, \\\n",
    "                                                    max_hour, max_load])\n",
    "        \n",
    "    return station_dict\n",
    "    \n",
    "station_dict = parse_file(datafile)\n",
    "    \n",
    "print(station_dict)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COAST', 2013, 8, 13, 17, 18779.025510000003]\n",
      "['EAST', 2013, 8, 5, 17, 2380.1654089999956]\n",
      "['FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024]\n",
      "['NORTH', 2013, 8, 7, 17, 1544.7707140000005]\n",
      "['NORTH_C', 2013, 8, 7, 18, 24415.570226999993]\n",
      "['SOUTHERN', 2013, 8, 8, 16, 5494.157645]\n",
      "['SOUTH_C', 2013, 8, 8, 18, 11433.30491600001]\n",
      "['WEST', 2013, 8, 7, 17, 1862.6137649999998]\n"
     ]
    }
   ],
   "source": [
    "out_file = '2013_Max_Loads.csv'\n",
    "\n",
    "def save_file(out_file):\n",
    "    with open(out_file, 'w') as f:\n",
    "        w = csv.writer(f, delimiter=\"|\")\n",
    "        w.writerow(['Station', 'Year' , 'Month' , 'Day', 'Hour', 'Max Load'])\n",
    "        for station, entry in station_dict.items():\n",
    "            station_data = [station] + entry\n",
    "            print(station_data)\n",
    "            w.writerow(station_data)\n",
    "        \n",
    "save_file(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
