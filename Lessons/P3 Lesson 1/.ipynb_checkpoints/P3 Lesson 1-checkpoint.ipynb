{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    data = [[sheet.cell_value(r,col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    \n",
    "    print('\\nList Comprehension')\n",
    "    print('data[4][3]:')\n",
    "    print(data[4][3])\n",
    "    \n",
    "    print('\\nCells in a nested loop:')\n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print(sheet.cell_value(row,col))\n",
    "                \n",
    "    print(\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "    print( \"Number of rows in the sheet:\" )\n",
    "    print( sheet.nrows)\n",
    "    print (\"Type of data in cell (row 3, col 2):\")\n",
    "    print( sheet.cell_type(3, 2))\n",
    "    print( \"Value in cell (row 3, col 2):\" )\n",
    "    print( sheet.cell_value(3, 2))\n",
    "    print( \"Get a slice of values in column 3, from rows 1-3:\")\n",
    "    print( sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "\n",
    "    print( \"\\nDATES:\")\n",
    "    print( \"Type of data in cell (row 1, col 0):\")\n",
    "    print( sheet.cell_type(1, 0))\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print( \"Time in Excel format:\")\n",
    "    print( exceltime)\n",
    "    print( \"Convert time to a Python datetime tuple, from the Excel float:\",)\n",
    "    print( xlrd.xldate_as_tuple(exceltime, 0))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    coast_data = sheet.col_values(1, 1)\n",
    "    max_load = max(coast_data)\n",
    "    max_index = coast_data.index(max_load)\n",
    "    max_time = sheet.cell_value(max_index + 1, 0)\n",
    "    max_time = xlrd.xldate_as_tuple(max_time, 0)\n",
    "    min_load = min(coast_data)\n",
    "    min_index = coast_data.index(min_load)\n",
    "    min_time = sheet.cell_value(min_index + 1, 0)\n",
    "    min_time = xlrd.xldate_as_tuple(min_time, 0)\n",
    "    average_load = sum(coast_data)/len(coast_data)\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': (0, 0, 0, 0, 0, 0),\n",
    "            'maxvalue': 0,\n",
    "            'mintime': (0, 0, 0, 0, 0, 0),\n",
    "            'minvalue': 0,\n",
    "            'avgcoast': 0\n",
    "    }\n",
    "    data['maxtime'] = max_time\n",
    "    data['maxvalue'] = round(max_load, 2)\n",
    "    data['mintime'] = min_time\n",
    "    data['minvalue'] = round(min_load, 2)\n",
    "    data['avgcoast'] = round(average_load, 2)\n",
    "    return data\n",
    "\n",
    "print(parse_file(datafile) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print( \"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print( json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"First Aid Kit\")\n",
    "    artists = results['artists']\n",
    "    counter = 0\n",
    "    for band in artists:\n",
    "        \n",
    "        if band['name'] == 'First Aid Kit':\n",
    "            print(band)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    # print( \"\\nARTIST:\")\n",
    "    #pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    # print( \"\\nONE RELEASE:\")\n",
    "    #pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    \n",
    "\n",
    "    # print( \"\\nALL TITLES:\")\n",
    "    for t in release_titles:\n",
    "        print(t)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = '745090.csv'\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'r') as f:\n",
    "        r = csv.reader(f, delimiter=\",\")\n",
    "        station = next(r)[1]\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            data.append(row)\n",
    "        \n",
    "    # Do not change the line below\n",
    "    name = station\n",
    "    return (name, data)\n",
    "\n",
    "parse_file(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COAST': [2013, 8, 13, 17, 18779.025510000003], 'EAST': [2013, 8, 5, 17, 2380.1654089999956], 'FAR_WEST': [2013, 6, 26, 17, 2281.2722140000024], 'NORTH': [2013, 8, 7, 17, 1544.7707140000005], 'NORTH_C': [2013, 8, 7, 18, 24415.570226999993], 'SOUTHERN': [2013, 8, 8, 16, 5494.157645], 'SOUTH_C': [2013, 8, 8, 18, 11433.30491600001], 'WEST': [2013, 8, 7, 17, 1862.6137649999998]}\n"
     ]
    }
   ],
   "source": [
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    station_dict = {}\n",
    "    for i in range(1, sheet.ncols-1):\n",
    "        \n",
    "        # Station names are in row 0, starting at column 1\n",
    "        station_name = sheet.cell_value(0, i)\n",
    "\n",
    "        # Retrieve the data, starts in column 1 row 1\n",
    "        # sheet.col_values(column, starting_row, ending_row=None)\n",
    "        data = sheet.col_values(i, 1)\n",
    "        \n",
    "        # Retrieve maximum load in the column (station)\n",
    "        max_load = max(data)\n",
    "        \n",
    "        # Retrieve index of maximum load (time)\n",
    "        max_index = data.index(max_load)\n",
    "        \n",
    "        # Retrieve time of maximum load (increase row by 1), convert time to tuple\n",
    "        # sheet.cell_value(row, column)\n",
    "        max_time = sheet.cell_value(max_index+1, 0)\n",
    "        max_time = xlrd.xldate_as_tuple(max_time, 0)\n",
    "        \n",
    "        max_year = max_time[0]\n",
    "        max_month = max_time[1]\n",
    "        max_day = max_time[2]\n",
    "        max_hour = max_time[3]\n",
    "        \n",
    "        station_dict[station_name] = ([max_year, max_month, max_day, \\\n",
    "                                                    max_hour, max_load])\n",
    "        \n",
    "    return station_dict\n",
    "    \n",
    "station_dict = parse_file(datafile)\n",
    "    \n",
    "print(station_dict)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COAST', 2013, 8, 13, 17, 18779.025510000003]\n",
      "['EAST', 2013, 8, 5, 17, 2380.1654089999956]\n",
      "['FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024]\n",
      "['NORTH', 2013, 8, 7, 17, 1544.7707140000005]\n",
      "['NORTH_C', 2013, 8, 7, 18, 24415.570226999993]\n",
      "['SOUTHERN', 2013, 8, 8, 16, 5494.157645]\n",
      "['SOUTH_C', 2013, 8, 8, 18, 11433.30491600001]\n",
      "['WEST', 2013, 8, 7, 17, 1862.6137649999998]\n"
     ]
    }
   ],
   "source": [
    "out_file = '2013_Max_Loads.csv'\n",
    "\n",
    "def save_file(out_file):\n",
    "    with open(out_file, 'w') as f:\n",
    "        w = csv.writer(f, delimiter=\"|\")\n",
    "        w.writerow(['Station', 'Year' , 'Month' , 'Day', 'Hour', 'Max Load'])\n",
    "        for station, entry in station_dict.items():\n",
    "            station_data = [station] + entry\n",
    "            print(station_data)\n",
    "            w.writerow(station_data)\n",
    "        \n",
    "save_file(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"3d73f2d953f44e178429dffd92c31a9d\",\n",
    "            \"article\": \"3d73f2d953f44e178429dffd92c31a9d\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_popular(URL_POPULAR, 'viewed', 1)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return data\n",
    "    # return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" :\n",
    "        print (\"You need to register for NYTimes Developer account to run this program.\")\n",
    "        print (\"See Intructor notes for information\")\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print (\"Time period can be 1,7, 30 days only\")\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print (\"kind can be only one of viewed/shared/emailed\")\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "        \n",
    "# titles, urls = article_overview(\"viewed\", 1)\n",
    "data = article_overview('viewed', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "urls = []\n",
    "# articles are dictionaries within data['results']\n",
    "for article in data['results']:\n",
    "    article_dict = {}\n",
    "    # If there is not already a key in the label dict with hte section, create one as empty list and add title\n",
    "    article_dict[article['section']] = article['title']\n",
    "    \n",
    "    labels.append(article_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data is a dictionary\n",
    "# article is a dictionary within data\n",
    "for article in data['results']:\n",
    "    # media is a dictionary within article['media']\n",
    "    media = article['media']\n",
    "    #element is a dictionary within media\n",
    "    for element in media:\n",
    "        # media_data is a dictionary within media\n",
    "        for media_data in element['media-metadata']:\n",
    "            if media_data['format'] == 'Standard Thumbnail':\n",
    "                urls.append(media_data['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Opinion': 'Donald, This I Will Tell You'},\n",
       " {'U.S.': 'After Barring Girls for Leggings, United Airlines Defends Decision'},\n",
       " {'Magazine': 'Trump vs. Congress: Now What?'},\n",
       " {'Opinion': 'I Loved My Grandmother. But She Was a Nazi.'},\n",
       " {'U.S.': 'Senate Committee to Question Jared Kushner Over Meetings With Russians'},\n",
       " {'U.S.': 'Trump Becomes Ensnared in Fiery G.O.P. Civil War'},\n",
       " {'World': 'Alone in the Wild for a Year, TV Contestants Learn Their Show Was Canceled'},\n",
       " {'World': 'Canadians Adopted Refugee Families for a Year. Then Came ‘Month 13.’'},\n",
       " {'Business Day': 'One Nation, Under Fox: 18 Hours With a Network That Shapes America'},\n",
       " {'World': 'Aleksei Navalny, Top Putin Critic, Arrested as Protests Flare in Russia'},\n",
       " {'Well': 'The Best Exercise for Aging Muscles'},\n",
       " {'U.S.': 'Jeanine Pirro Calls for Paul Ryan to Step Down After Health Bill Failure'},\n",
       " {'World': 'Who Killed the Iceman? Clues Emerge in a Very Cold Case'},\n",
       " {'U.S.': 'Dealt a Defeat, Republicans Set Their Sights on Major Tax Cuts'},\n",
       " {'U.S.': 'Boris Epshteyn, Trump TV Surrogate, Is Leaving White House Job'},\n",
       " {'Opinion': 'Trump’s Triumph of Incompetence'},\n",
       " {'Opinion': 'Break Up the Liberal City'},\n",
       " {'U.S.': 'Who Stopped the Republican Health Bill?'},\n",
       " {'N.Y. / Region': 'Things I Wish I Had Known When My Dog Died'},\n",
       " {'U.S.': 'Paul Ryan Emerges From Health Care Defeat Badly Damaged'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
