{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'email': 'omer@extremegate.com',\n",
       "  'fnm': 'Omer',\n",
       "  'insr': ['I1'],\n",
       "  'snm': 'Mei-Dan'},\n",
       " {'email': 'mcarmont@hotmail.com',\n",
       "  'fnm': 'Mike',\n",
       "  'insr': ['I2'],\n",
       "  'snm': 'Carmont'},\n",
       " {'email': 'laver17@gmail.com',\n",
       "  'fnm': 'Lior',\n",
       "  'insr': ['I3', 'I4'],\n",
       "  'snm': 'Laver'},\n",
       " {'email': 'nyska@internet-zahav.net',\n",
       "  'fnm': 'Meir',\n",
       "  'insr': ['I3'],\n",
       "  'snm': 'Nyska'},\n",
       " {'email': 'kammarh@gmail.com',\n",
       "  'fnm': 'Hagay',\n",
       "  'insr': ['I8'],\n",
       "  'snm': 'Kammar'},\n",
       " {'email': 'gideon.mann.md@gmail.com',\n",
       "  'fnm': 'Gideon',\n",
       "  'insr': ['I3', 'I5'],\n",
       "  'snm': 'Mann'},\n",
       " {'email': 'barns.nz@gmail.com',\n",
       "  'fnm': 'Barnaby',\n",
       "  'insr': ['I6'],\n",
       "  'snm': 'Clarck'},\n",
       " {'email': 'eukots@gmail.com', 'fnm': 'Eugene', 'insr': ['I7'], 'snm': 'Kots'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        data['fnm'] = author.find('./fnm').text\n",
    "        data['snm'] = author.find('./snm').text\n",
    "        data['email'] = author.find('./email').text\n",
    "        \n",
    "        for element in author.findall('./insr'):\n",
    "            data['insr'].append(element.get('iid'))\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "get_authors(get_root(article_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_source = 'page_source.html'\n",
    "\n",
    "soup = BeautifulSoup(open(page_source), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "\n",
    "event_validation = soup.find(id='__EVENTVALIDATION')\n",
    "data['eventvalidation'] = event_validation.get('value')\n",
    "\n",
    "view_state = soup.find(id='__VIEWSTATE')\n",
    "data['viewstate'] = view_state.get('value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_page = 'options.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FL',\n",
       " 'AS',\n",
       " 'AA',\n",
       " 'MQ',\n",
       " '5Y',\n",
       " 'DL',\n",
       " 'EV',\n",
       " 'F9',\n",
       " 'HA',\n",
       " 'B6',\n",
       " 'OO',\n",
       " 'WN',\n",
       " 'NK',\n",
       " 'US',\n",
       " 'UA',\n",
       " 'VX']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_carriers(page):\n",
    "    carriers = []\n",
    "    soup = BeautifulSoup(open(page), 'lxml')\n",
    "    carrier_list = soup.find(id='CarrierList')\n",
    "    for carrier in carrier_list.find_all('option'):\n",
    "        if len(carrier['value']) == 2:\n",
    "            carriers.append(carrier['value'])\n",
    "            \n",
    "    return carriers\n",
    "\n",
    "get_carriers(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATL',\n",
       " 'BWI',\n",
       " 'BOS',\n",
       " 'CLT',\n",
       " 'MDW',\n",
       " 'ORD',\n",
       " 'DFW',\n",
       " 'DEN',\n",
       " 'DTW',\n",
       " 'FLL',\n",
       " 'IAH',\n",
       " 'LAS',\n",
       " 'LAX',\n",
       " 'ABR',\n",
       " 'ABI']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_airports(page):\n",
    "    airports = []\n",
    "    soup = BeautifulSoup(open(page), 'lxml')\n",
    "    airport_list = soup.find(id='AirportList')\n",
    "    for airport in airport_list.find_all('option'):\n",
    "        if (len(airport['value']) ==3) and (airport['value'] != 'All'):\n",
    "            airports.append(airport['value'])\n",
    "            \n",
    "    return airports\n",
    "\n",
    "get_airports(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_file = 'FL-ATL.html'\n",
    "courier = html_file[0:2]\n",
    "airport = html_file[3:6]\n",
    "\n",
    "data = []\n",
    "def process_file(file):\n",
    "    # Data will be a list of dictionaries where each dictionary represents one row of the table\n",
    "    data = []\n",
    "    # Record courier and airport code (assuming file names are constant)\n",
    "    courier = file[0:2]\n",
    "    airport = file[3:6]\n",
    "    \n",
    "    # Put the courier and airport into a dictionary \n",
    "    airport_data = {'courier' : courier, 'airport': airport}\n",
    "    \n",
    "    # Create the soup for parsing\n",
    "    soup = BeautifulSoup(open(file), 'lxml')\n",
    "    \n",
    "    # Finds every instance of table row that equals the right class\n",
    "    # table_data contains all the rows\n",
    "    table_data  = soup.find_all('tr', class_='dataTDRight')\n",
    "    \n",
    "    # Iterating through the table data one row at a time, each row will become its own dictionary\n",
    "    for row in table_data:\n",
    "        # month_data is dictionary for each row\n",
    "        month_data = {}\n",
    "        \n",
    "        # do not want to include the totals\n",
    "        if row.find_all('td')[1].text == 'TOTAL':\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            # flight data will contain the domestic and international flights\n",
    "            flight_data = {}\n",
    "            month_data = airport_data\n",
    "            \n",
    "            table_row = []\n",
    "            \n",
    "            # Iterate through the data entries in the row\n",
    "            for data_entry in row.find_all('td'):\n",
    "                # Clean each data entry, can access the information stored under the <td> tag using string\n",
    "                # This removes the commas\n",
    "                clean_data = int(data_entry.text.replace(',', ''))\n",
    "\n",
    "                # Add the cleaned data entry into the row list\n",
    "                table_row.append(clean_data)\n",
    "               \n",
    "            # Extract the entries from the row list and place in appropriate location\n",
    "            month_data['year'] = table_row[0]\n",
    "            month_data['month'] = table_row[1]\n",
    "            flight_data['domestic'] = table_row[2]\n",
    "            flight_data['international'] = table_row[3]\n",
    "            \n",
    "            month_data['flights'] = flight_data\n",
    "            \n",
    "            data.append(month_data.copy())\n",
    "            \n",
    "    return data\n",
    "\n",
    "data = process_file(html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 815489, 'international': 92565},\n",
       "  'month': 10,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766775, 'international': 91342},\n",
       "  'month': 11,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 782175, 'international': 96881},\n",
       "  'month': 12,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 785651, 'international': 98053},\n",
       "  'month': 1,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 690750, 'international': 85965},\n",
       "  'month': 2,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 797634, 'international': 97929},\n",
       "  'month': 3,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766639, 'international': 89398},\n",
       "  'month': 4,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 789857, 'international': 87671},\n",
       "  'month': 5,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 798841, 'international': 95435},\n",
       "  'month': 6,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 832075, 'international': 102795},\n",
       "  'month': 7,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 831185, 'international': 102145},\n",
       "  'month': 8,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 782264, 'international': 90681},\n",
       "  'month': 9,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 818777, 'international': 91820},\n",
       "  'month': 10,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766266, 'international': 91004},\n",
       "  'month': 11,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 798879, 'international': 97094},\n",
       "  'month': 12,\n",
       "  'year': 2003}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
