{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenStreetMap](https://www.openstreetmap.org/#map=11/41.4980/-81.7070) is a user-generated map of the entire world, freely available to download. \n",
    "The data extract of Cleveland used for this project was downloaded from [Mapzen Metro Extracts](https://mapzen.com/data/metro-extracts/metro/cleveland_ohio/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to download the map as an XML file. The original file was nearly 5.5 million lines long, which meant that it would be unwieldy to process the entire file every time. I decided to create sample files for testing out my auditing scripts and cleaning. I wanted a smaller file to test run functions and then an intermediate file I could use for identifying the most common problems prior to cleaning the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orginal file contains the following breakdown of top-level elements: {'way': 189248, 'node': 1795742, 'relation': 3732}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The breakdown of top-level elements for the three file sizes is summarized in the following table:\n",
    "\n",
    "|                     \t    | 'node'  \t| 'way'  \t| 'relation' \t| File Size (MB) \t|\n",
    "|-----------------------\t|---------\t|--------\t|------------\t|----------------\t|\n",
    "| __Full file__          \t| 1795742 \t| 189248 \t| 3732       \t|      392.6     \t|\n",
    "| __Intermediate sample__ \t| 179575  \t| 18924  \t| 374        \t|      39.9      \t|\n",
    "| __Small sample__        \t| 17958   \t| 1892   \t| 38         \t|       3.9      \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Approach\n",
    "Being in possession of a large dataset can be both exciting and intimidating! There are numerous possiblities, but the sheer amount of data can be overwhelming. Before I officially began any data wrangling, I wanted to have a step-by-step plan to guide my actions in order to efficiently go through the process of cleaning the data. I decided that the following approach, adapted from the Udacity course on Data Wrangling was a solid guide:\n",
    "\n",
    "1. Audit the data: identify errors/missing data in the XML data\n",
    "2. Create a data cleaning plan based on the audit\n",
    "    * Identify the causes of any \"dirty\" or inconsistent data \n",
    "    * Develop a set of corrective cleaning actions and test on a small sample of the XML data\n",
    "3. Implement the data cleaning plan: run scripts and transfer the cleaned data to .csv files\n",
    "4. Manually correct as necessary: import the data from .csv to SQL and perform SQL queries on the data to identify any further inconsistencies that would necessitate returning to step 2. \n",
    "\n",
    "Data wrangling is an iterative procedure, and as such, I expected that I might need to cycle through these steps several times. However, I knew that having a clear outline of the procedure to follow would save me untold hours of work and confusion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five main aspects of data quality to consider when auditing a dataset:\n",
    "1. Validity: Does the data conform to a schema (standard format)?\n",
    "2. Accuracy: Does the data conform to reality or a trusted external source?\n",
    "3. Completeness: Are all records present?\n",
    "4. Consistency: Is data in a field or across a row in logical agreement?\n",
    "5. Uniformity: Are the same units used for a given field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_file = 'amenitiesSource.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After examining the data and doing some preliminary audits, I identified six elements of the data set that I could clean before exporting the data to CSV files to be imported into a SQL database. \n",
    "\n",
    "### Auditing and Cleaning\n",
    "\n",
    "#### 1. Elevation\n",
    "\n",
    "Elevation: Looking at the official documentation for [elevation data in OpenStreetMap](http://wiki.openstreetmap.org/wiki/Altitude), elevation in tags should be recorded in meters. Furthermore, from the [United States Geological Survey list of elevation of major cities](https://egsc.usgs.gov/isb//pubs/booklets/elvadist/elvadist.html), I can see that the elevation of Cleveland ranges from 571' at the lowest point to 1050' at the highest point. Therefore, after converting to meters, all elevation data encoded in node tags should be between 174 m and 320 m. This would be a test for both uniformity, because all elevation data should use the same units, and accuracy as I was checking the elevations against a trusted source. \n",
    "The audit_elevations.py script identified any elevations above or below those measures, as well as any elevations that could not be cast to an integer. I could have used regular expressions to identify problematic characters, but since I expected all the elevations to be integers, the simpler approach was to attempt to cast all the elevation data to integers. This initial effort yielded 2516 suspect elevations and 5 problem elevations in the entire dataset. I decided that assuming all the elevations above or below the accepted limits were false was a mistake. The USGS data was for the city itself, and my map included surrounding areas. Therefore, I took a closer look at the suspect elevations. The four highest values were [1188, 423, 419, 414] and the four lowest were [3, 3, 42, 321]. I decided that 423 m was a reasonable elevation for the surrounding area of Cleveland. However, clearly the 1188 was meant to be in feet, and the three lowest points were not realistic. I made the choice to eliminate the three lowest elevation tags and convert the largest value to meters. The elevations with bad characters were ['318;313', '304.5', '298.4', '679\"', '348;352;358']. I decided to take the first elevation listed if their were multiple ones and convert the one in feet to meters. Total, this meant I only had to correct 9 problem elevations out of 6500. That makes me confident that the accuracy and uniformity of the dataset was initially very high. \n",
    "\n",
    "The actual cleaning of the elevation data would take place when I converted the XML into CSV files. The script for auditing and cleaning the elevation data is below. \n",
    "\n",
    "```python\n",
    "# audit_elevation.py\n",
    "import xml.etree.cElementTree as ET\n",
    "import heapq\n",
    "\n",
    "map_file = 'cleveland_ohio.xml'\n",
    "\n",
    "suspect_elevations = []\n",
    "bad_elevations = []\n",
    "\n",
    "elevation_mapping = {'3': '',\n",
    "\t\t\t\t\t '42': '',\n",
    "\t\t\t\t\t '1188' : '362',\n",
    "\t\t\t\t\t '318;313' : '318',\n",
    "\t\t\t\t\t '304.5' : '305',\n",
    "\t\t\t\t\t '298.4' : '298',\n",
    "\t\t\t\t\t '679\"': '207',\n",
    "\t\t\t\t\t '348;352;358' : '348'}\n",
    "\n",
    "count = 0\n",
    "def audit():\n",
    "\tfor _, elem in ET.iterparse(map_file):\n",
    "\t\tif elem.tag == 'tag':\n",
    "\t\t\tif elem.attrib['k'] == 'ele':\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\televation = int(elem.attrib['v'])\n",
    "\t\t\t\t\tif elevation > 320 or elevation < 124:\n",
    "\t\t\t\t\t\tsuspect_elevations.append(elevation)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tbad_elevations.append(elem.attrib['v'])\n",
    "\n",
    "\n",
    "def clean(elevation):\n",
    "\tif elevation in elevation_mapping:\n",
    "\t\treturn elevation_mapping[elevation]\n",
    "        \n",
    "```\n",
    "\n",
    "The result of running the code on the problem elevations is:\n",
    "```\n",
    "3 --> \n",
    "3 --> \n",
    "42 --> \n",
    "1188 --> 362\n",
    "318;313 --> 318\n",
    "304.5 --> 305\n",
    "298.4 --> 298\n",
    "679\" --> 207\n",
    "348;352;358 --> 348\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amenity tag associated with the way elements was one facet of the data I could audit for validity. The OpenStreetMap wiki has a [page on the amenity key](http://wiki.openstreetmap.org/wiki/Key:amenity). This webpage shows all of the accepted amenities in OpenStreetMap and gives the purpose of the tag: \"To cover an assortment of community facilities including toilets, telephones, banks, pharmacies and schools.\" I wanted to check if all the amenities tags in the Cleveland dataset were among the officially validated tags from OpenStreetMap. To do so, I downloaded the source of the webpage as XML and extracted all of the amentities from the table using the XML module. I then compared the amentities in the Cleveland OpenStreetMap XML file to this set. I did not necessarily want to delete the amentities that could not be validated, but rather, I would reclassify them into the amenity categories already part of OpenStreetMap. The non-verified amentities and the number of times they occured in the dataset are presented below:\n",
    "\n",
    "```python\n",
    "{'childcare': 1, 'public_building': 1, 'Court House': 1, 'Government': 1, 'funeral_home': 1, 'gym': 1, 'swimming_pool': 1, 'music_venue': 1, 'bail_bonds': 1, 'social_club': 1, 'bell': 1, 'compressed_air': 1, 'hackerspace': 1, 'raspberries': 1, 'picnic_table': 1, 'child_care': 1, 'boat_rental': 1, 'library;theatre': 1, 'trade_school': 1, 'grill': 1, 'slipway': 1, 'winery': 1, 'boat_storage': 1, 'dog_park': 1, 'park': 1, 'nursery': 1, 'retirement_home': 1, 'printer': 1, 'hospice': 1}\n",
    "```\n",
    "\n",
    "As can be seen, none of the non-verified amenities occured more than once and several were not in agreement with the purpose of the tag from the documentation. Therefore, I concluded that it would be best to reclassify the amenities that could not be verified according to the official dichotomy rather than modify the official list. I created a mapping based on the official description of the amenities and applied it when converting the XML data to CSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Street Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another audit I could perform for validity, or whether or not the data adheres to a standard schema, concerned the street names associated with the node and way tags. From my initial exploratory examination of the data, I had noticed a wide variety in name endings and abbreviations. Using a provisional audit_street_names Python script, I compared the street endings to a standardized list and counted the number of times each non-standard type appeared. That produced the following dictionary:\n",
    "\n",
    "```python\n",
    "{'ave': 1, 'Rd': 29, 'NW': 6, 'St.': 5, 'Ave.': 9, 'NE': 2, 'Northeast': 15, 'Circle': 9, 'Rauscher': 1, 'Blvd': 10, 'St': 10, 'Blvd.': 2, 'Cedar': 1, 'Arlington': 1, 'Engel': 1, 'Fleet': 1, 'Ave': 9, 'Center': 1, 'Rd.': 3, 'Ln': 1, 'Mayfield': 1, 'East': 7, 'Clair': 1, 'North': 1, 'Southeast': 8, 'Lee': 1, 'Northwest': 16, 'West': 4, 'Path': 3, 'Smith': 1, '106': 1, 'Pkwy': 2, 'South': 2, 'Extension': 1, 'Shoreway': 1, 'Fairgrounds': 1, 'Ravenna': 1, 'Dr': 6, 'SE': 3, '303': 1, 'Southwest': 4, 'W': 2, 'Pike': 1, 'OH-14': 1, 'Parschen': 1, '534': 1, '3': 1, 'OH-18': 1, '6': 1, 'B': 2, '6A': 1, '100': 1, 'A': 2, '10': 1, 'Way': 6, 'st': 2, '250N': 1, 'SW': 1, 'Middleton': 1, '150th': 1, 'Lorain': 1, 'Esplanade': 2, '14': 2, 'Ridge': 1, '43': 1, 'Dr.': 1, '20': 1, 'Cut': 1, 'Plaza': 1, 'Soutwest': 1, '1175': 1, 'Terrace': 1, 'Hill': 1, '88': 1, '336': 1, '1102': 1, 'rd': 1, 'N.E.': 1, 'Downs': 1, 'Street.': 1, 'Paula': 1}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to standardize the street names, I created the following mapping and applied it to the data during the conversion from XML to CSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another audit I could perform for accuracy was zip/postal codes. I could find a list of all the postal codes in the area I had downloaded, and then compare those to what was recorded in the XML file. The first step was to find all the post codes. I initially searched Cleveland Zip Codes and used the United States Postal Service data for postal codes in Cleveland. However, I soon realized that the area I had downloaded was a square and did not necessarily conform to the administrative boundaries of the city of Cleveland. My next approach was to examine the XML I had downloaded and find the latitude and longitude boudaries of the square. Then, using the [FreeMapTools](https://www.freemaptools.com/find-zip-codes-inside-user-defined-area.htm) zip code finding tool, I was able to draw the exact box on the map and extract all the zip codes within the specific box. This yielded me 276 zip codes. I then compared all of the zip codes within the XML data to this set and extracted those that did not match as well as any problem zip codes such as {'44256:44321'} that could not be converted to an integer. For the problem zip codes if there were multiple codes separated by a colon, I took the first zip code as long as it was in the official list. For the zip codes outside of the geographic area, I decided to remove the tag entirely. After performing the aduit, I had 39 zip codes outside of the boundary box and 31 zip codes that included multiple codes.  The following code was used to perform the actual cleaning of the zip codes:\n",
    "\n",
    "``` python\n",
    "\n",
    "# Takes in a zip_code as a string and returns zip_code as a string\n",
    "def clean(zip_code):\n",
    "\n",
    "\tif len(zip_code) > 5:\n",
    "        # Use a regular expression to split the zip code on : or ;\n",
    "\t\tzip_code = re.split('[;:]', zip_code)\n",
    "        \n",
    "        # If the first zip code is in the Cleveland zip codes, use it\n",
    "\t\tif int(zip_code[0]) in cleveland_zip_codes:\n",
    "\t\t\tzip_code = zip_code[0]\n",
    "           \n",
    "        # Otherwise use the second code if it is in the Cleveland zip codes\n",
    "\t\telif int(zip_code[-1]) in cleveland_zip_codes:\n",
    "\t\t\tzip_code = zip_code[-1]\n",
    "\t\telse:\n",
    "\t\t\tzip_code = ''\n",
    "            \n",
    "\telse: \n",
    "\t\tif int(zip_code) not in cleveland_zip_codes:\n",
    "\t\t\tzip_code = '' # Return an empty string so the tag will not be included in the list of tags\n",
    "\n",
    "\treturn zip_code\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Dates\n",
    "\n",
    "The last main issue I observed in my audit of the data was the inconsistent date formats in use. All the timestamps automatically recorded in the OpenStreetMap data were recorded as: timestamp=\"2009-05-21T03:59:40Z\". However, some of the dates under the secondary elements were in other formats such as v=\"06/05/2006\". Dates such as this can be intrepreted ambiguously, and I wanted all dates to be recorded in a consistent format so they could be more easily found and understood in any data analysis performed on the set. In the entire Cleveland XML source data, there were 16320 dates in the MM/DD/YEAR format. Looking through the documentation for OpenStreetMap [concerning dates](http://wiki.openstreetmap.org/wiki/Key:source:date), I found the following statement: \"There is no standing recommendation as to the date format to be used. However, the international standard ISO 8601 appears to be followed by 9 of the top 10 values for this tag. The ISO 8601 basic date format is YYYY-MM-DD.\" Based on this information, I decided to adopt the [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html) format for dates. The cleaning function to convert dates from MM/DD/YEAR to YEAR-MM-DD is below:\n",
    "\n",
    "```python\n",
    "\n",
    "# Takes in a date, checks the format, and returns date in YEAR-MM-DD\n",
    "def clean(date):\n",
    "\ttry:\n",
    "\t\tdate = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "\t\treturn date\n",
    "\texcept:\n",
    "\t\tdate = datetime.datetime.strptime(date, '%m/%d/%Y').date()\n",
    "\t\treturn date.strftime('%Y-%m-%d')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sql Database Creation\n",
    "\n",
    "The initial cleaning procedure was completed when the data was converted from the original source XML file to eight different CSV files. However, I was expected that in creating and exploring the SQL database, I would find more \"dirty\" data that I would need to correct. I was well aware that data wrangling is an iterative procedure, and so was ready to work through the steps of the process again if needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Amenities\n",
    "3. Zip codes\n",
    "4. Street-names\n",
    "5. GNIS created and edited dates\n",
    "6. Latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists\n"
     ]
    }
   ],
   "source": [
    "amenity ='1'\n",
    "if amenity:\n",
    "    print('exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does not exist\n"
     ]
    }
   ],
   "source": [
    "date = ''\n",
    "if not date:\n",
    "    print('does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
